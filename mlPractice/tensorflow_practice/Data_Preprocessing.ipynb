{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f607a4-9658-4e90-901b-89264fbbf47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imseongbin/miniforge3/envs/ml_for_everyone/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 22:53:45.962071: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-17 22:53:45.962338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_ds = tfds.load(name='mnist',\n",
    "                     shuffle_files=True,\n",
    "                     as_supervised=True,\n",
    "                     split='train',\n",
    "                     batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb04c6f-8d9a-4e3b-9b8e-574f8c11d959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 28, 28, 1)\n",
      "<dtype: 'float32'>\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 22:58:46.541985: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds:\n",
    "    print(images.shape)\n",
    "    print(images.dtype) # float32가 아닌 uint8타입\n",
    "    \n",
    "    print(tf.reduce_max(images)) # 최대값이 255 ~ 값이 매우 높다\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35df94e3-04ce-4d5b-8ab9-5be1d6d28b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'uint8'> tf.Tensor(255, shape=(), dtype=uint8)\n",
      "<dtype: 'float32'> tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 22:58:42.657178: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-05-17 22:58:42.725450: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tfds.load(name='mnist',\n",
    "                     shuffle_files=True,\n",
    "                     as_supervised=True,\n",
    "                     split='train',\n",
    "                     batch_size=4)\n",
    "\n",
    "def standardization(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.\n",
    "    return images, labels\n",
    "\n",
    "train_ds_iter = iter(train_ds)\n",
    "images, labels = next(train_ds_iter)\n",
    "print(images.dtype, tf.reduce_max(images))\n",
    "\n",
    "\n",
    "# 테스트 데이터에서도 동일하게 적용해줘야한다.\n",
    "train_ds = train_ds.map(standardization)\n",
    "\n",
    "train_ds_iter = iter(train_ds)\n",
    "images, labels = next(train_ds_iter)\n",
    "print(images.dtype, tf.reduce_max(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84710b1-39b3-4ad6-8efd-d8bdf150b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'> tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 23:00:53.311797: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# 재사용성 있게 함수로~\n",
    "def mnist_data_loader():\n",
    "    (train_ds, test_ds) = tfds.load(name='mnist',\n",
    "                     shuffle_files=True,\n",
    "                     as_supervised=True,\n",
    "                     split=['train', 'test'],\n",
    "                     batch_size=4)\n",
    "    def standardization(images, labels):\n",
    "        images = tf.cast(images, tf.float32) / 255.\n",
    "        return images, labels\n",
    "    \n",
    "    train_ds = train_ds.map(standardization)\n",
    "    test_ds = test_ds.map(standardization)\n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "train_ds, test_ds = mnist_data_loader()\n",
    "\n",
    "train_ds_iter = iter(train_ds)\n",
    "images, labels = next(train_ds_iter)\n",
    "print(images.dtype, tf.reduce_max(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd14e3-97d2-4a98-9858-baf521716509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_for_everyone",
   "language": "python",
   "name": "ml_for_everyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
