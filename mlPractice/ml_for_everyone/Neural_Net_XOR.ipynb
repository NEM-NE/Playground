{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "efff026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bbc8656f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQZ0lEQVR4nO3dXYxdV3nG8f8TmxBaAlR4kJBtcJo6Km6KSjqkqVBLaGjl5MJWBUW2FL6UYgkaWhWEmpY2ULs3NCqVkNKCEUkgLYRAJTICU19AUCTAqSdKibDT0KkxZIKjDCFESGmYOHl7cY7LYTxjj+3Z5+R4/X+SNftjzdnvmo/9eO11Zu9UFZKkdp0z6gIkSaNlEEhS4wwCSWqcQSBJjTMIJKlxq0ddwKlas2ZNbdiwYdRlSNJYueeee35YVROL7Ru7INiwYQPT09OjLkOSxkqS7y21z0tDktQ4g0CSGmcQSFLjDAJJapxBIEmN6ywIktyU5JEk315if5J8JMlMkvuSXNJVLQBHjsCFF8LDD3d5FEnqSIcnsS5HBLcAm0+w/0pgY//fDuCfO6yFXbvg8OHeR0kaOx2exDoLgqq6C/jRCZpsBT5VPfuAFyV5aRe1HDkCN98MzzzT++ioQNJY6fgkNso5grXAgwPrs/1tx0myI8l0kum5ublTPtCuXb2vH8DTTzsqkDRmOj6JjcVkcVXtrqrJqpqcmFj0L6SXdCxI5+d76/PzjgokjZEhnMRGGQQPAesH1tf1t62owSA9xlGBpLExhJPYKINgCnhL/91DlwGPV9WRFT/I1M+C9Jj5ebjjjpU+kiR1YAgnsc5uOpfkM8DlwJoks8AHgOcAVNVHgT3AVcAM8ATw9i7qmJ3t4lUlaUiGcBLrLAiqavtJ9hfwJ10dX5K0PGMxWSxJ6o5BIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhrXaRAk2ZzkgSQzSa5bZP/LktyZ5N4k9yW5qst6JEnH6ywIkqwCbgSuBDYB25NsWtDsr4Hbq+pVwDbgn7qqR5K0uC5HBJcCM1V1qKrmgduArQvaFPCC/vILgR90WI8kaRFdBsFa4MGB9dn+tkEfBK5OMgvsAd692Asl2ZFkOsn03NxcF7VKUrNGPVm8HbilqtYBVwG3JjmupqraXVWTVTU5MTEx9CIl6WzWZRA8BKwfWF/X3zboGuB2gKr6JnAesKbDmiRJC3QZBPuBjUkuSHIuvcngqQVtvg9cAZDkFfSCwGs/kjREnQVBVR0FrgX2AvfTe3fQgSQ7k2zpN3sv8I4k3wI+A7ytqqqrmiRJx1vd5YtX1R56k8CD264fWD4IvKbLGiRJJzbqyWJJ0ogZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjes0CJJsTvJAkpkk1y3R5k1JDiY5kOTTXdYjSTre6q5eOMkq4Ebg94FZYH+Sqao6ONBmI/CXwGuq6rEkL+mqHknS4rocEVwKzFTVoaqaB24Dti5o8w7gxqp6DKCqHumwHknSIroMgrXAgwPrs/1tgy4CLkry9ST7kmxe7IWS7EgynWR6bm6uo3IlqU2jnixeDWwELge2Ax9P8qKFjapqd1VNVtXkxMTEcCuUpLNcl0HwELB+YH1df9ugWWCqqp6qqu8C36EXDJKkIekyCPYDG5NckORcYBswtaDNF+iNBkiyht6lokMd1iRJWqCzIKiqo8C1wF7gfuD2qjqQZGeSLf1me4FHkxwE7gTeV1WPdlWTJOl4qapR13BKJicna3p6etRlSNJYSXJPVU0utm/Uk8WSpBEzCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ17oRBkOQFSS5cZPsruytJkjRMSwZBkjcB/wX8W//B8q8e2H1L14VJkobjRCOCvwJ+s6p+A3g7cGuSP+zvS9eFSZKGY/UJ9q2qqiMAVfUfSV4HfDHJemC87l0tSVrSiUYEPxmcH+iHwuXAVuDXOq5LkjQkJwqCdwLnJNl0bENV/QTYDPxx14VJkoZjySCoqm9V1X8Dtyf5i/Q8D/gw8K6hVShJ6tRy/o7gt4D1wDfoPZD+B8BruixKkjQ8ywmCp4D/BZ4HnAd8t6qe6bQqSdLQLCcI9tMLglcDvwNsT/K5TquSJA3Nid4+esw1VTXdXz4CbE3y5g5rkiQN0UlHBAMhMLjt1m7KkSQNmzedk6TGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDWu0yBIsjnJA0lmklx3gnZvSFJJJrusR5J0vM6CIMkq4EbgSmATvXsUbVqk3fnAnwF3d1WLJGlpXY4ILgVmqupQVc0Dt9F7utlCu4APAU92WIskaQldBsFa4MGB9dn+tv+X5BJgfVV96UQvlGRHkukk03NzcytfqSQ1bGSTxUnOofe0s/eerG1V7a6qyaqanJiY6L44SWpIl0HwEL0nmx2zrr/tmPOBi4GvJTkMXAZMOWEsScPVZRDsBzYmuSDJucA2YOrYzqp6vKrWVNWGqtoA7AO2LHbba0lSdzoLgqo6ClwL7AXuB26vqgNJdibZ0tVxJUmnZjlPKDttVbUH2LNg2/VLtL28y1okSYvzL4slqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4zoNgiSbkzyQZCbJdYvsf0+Sg0nuS/KVJC/vsh5J0vE6C4Ikq4AbgSuBTcD2JJsWNLsXmKyqVwKfB/6+q3okSYvrckRwKTBTVYeqah64Ddg62KCq7qyqJ/qr+4B1HdYjSVpEl0GwFnhwYH22v20p1wBfXmxHkh1JppNMz83NrWCJkqRnxWRxkquBSeCGxfZX1e6qmqyqyYmJieEWJ0lnudUdvvZDwPqB9XX9bT8nyeuB9wOvraqfdliPJGkRXY4I9gMbk1yQ5FxgGzA12CDJq4CPAVuq6pEOa5EkLaGzIKiqo8C1wF7gfuD2qjqQZGeSLf1mNwDPBz6X5D+TTC3xcpKkjnR5aYiq2gPsWbDt+oHl13d5fEnSyT0rJoslSaNjEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGdRoESTYneSDJTJLrFtn/3CSf7e+/O8mGzoo5cgQuvBAefrizQ0hSV7o8hXUWBElWATcCVwKbgO1JNi1odg3wWFX9CvCPwIe6qoddu+Dw4d5HSRozXZ7CuhwRXArMVNWhqpoHbgO2LmizFfhkf/nzwBVJsuKVHDkCN98MzzzT++ioQNIY6foU1mUQrAUeHFif7W9btE1VHQUeB1688IWS7EgynWR6bm7u1CvZtav3FQR4+mlHBZLGStensLGYLK6q3VU1WVWTExMTp/bJx6J0fr63Pj/vqEDS2BjGKazLIHgIWD+wvq6/bdE2SVYDLwQeXdEqBqP0GEcFksbEME5hXQbBfmBjkguSnAtsA6YWtJkC3tpffiPw1aqqFa1iaupnUXrM/DzccceKHkaSujCMU9jqlXupn1dVR5NcC+wFVgE3VdWBJDuB6aqaAj4B3JpkBvgRvbBYWbOzK/6SkjQswziFdRYEAFW1B9izYNv1A8tPAn/UZQ2SpBMbi8liSVJ3DAJJapxBIEmNMwgkqXFZ6Xdrdi3JHPC90/z0NcAPV7CccWCf22Cf23AmfX55VS36F7ljFwRnIsl0VU2Ouo5hss9tsM9t6KrPXhqSpMYZBJLUuNaCYPeoCxgB+9wG+9yGTvrc1ByBJOl4rY0IJEkLGASS1LizMgiSbE7yQJKZJNctsv+5ST7b3393kg0jKHNFLaPP70lyMMl9Sb6S5OWjqHMlnazPA+3ekKSSjP1bDZfT5yRv6n+vDyT59LBrXGnL+Nl+WZI7k9zb//m+ahR1rpQkNyV5JMm3l9ifJB/pfz3uS3LJGR+0qs6qf/Ruef0/wC8D5wLfAjYtaPMu4KP95W3AZ0dd9xD6/DrgF/rL72yhz/125wN3AfuAyVHXPYTv80bgXuCX+usvGXXdQ+jzbuCd/eVNwOFR132Gff5d4BLg20vsvwr4MhDgMuDuMz3m2TgiuBSYqapDVTUP3AZsXdBmK/DJ/vLngSuSZIg1rrST9rmq7qyqJ/qr++g9MW6cLef7DLAL+BDw5DCL68hy+vwO4Maqegygqh4Zco0rbTl9LuAF/eUXAj8YYn0rrqruovd8lqVsBT5VPfuAFyV56Zkc82wMgrXAgwPrs/1ti7apqqPA48CLh1JdN5bT50HX0PsfxTg7aZ/7Q+b1VfWlYRbWoeV8ny8CLkry9ST7kmweWnXdWE6fPwhcnWSW3vNP3j2c0kbmVH/fT6rTB9Po2SfJ1cAk8NpR19KlJOcAHwbeNuJShm01vctDl9Mb9d2V5Ner6sejLKpj24Fbquofkvw2vaceXlxVz5zsE9VzNo4IHgLWD6yv629btE2S1fSGk48OpbpuLKfPJHk98H5gS1X9dEi1deVkfT4fuBj4WpLD9K6lTo35hPFyvs+zwFRVPVVV3wW+Qy8YxtVy+nwNcDtAVX0TOI/ezdnOVsv6fT8VZ2MQ7Ac2Jrkgybn0JoOnFrSZAt7aX34j8NXqz8KMqZP2OcmrgI/RC4Fxv24MJ+lzVT1eVWuqakNVbaA3L7KlqqZHU+6KWM7P9hfojQZIsobepaJDQ6xxpS2nz98HrgBI8gp6QTA31CqHawp4S//dQ5cBj1fVkTN5wbPu0lBVHU1yLbCX3jsObqqqA0l2AtNVNQV8gt7wcYbepMy20VV85pbZ5xuA5wOf68+Lf7+qtoys6DO0zD6fVZbZ573AHyQ5CDwNvK+qxna0u8w+vxf4eJI/pzdx/LZx/o9dks/QC/M1/XmPDwDPAaiqj9KbB7kKmAGeAN5+xscc46+XJGkFnI2XhiRJp8AgkKTGGQSS1DiDQJIaZxBIUuMMAmkFJfn3JD9O8sVR1yItl0EgrawbgDePugjpVBgE0mlI8ur+veDPS/KL/Xv/X1xVXwF+Mur6pFNx1v1lsTQMVbU/yRTwd8DzgH+pqkUfJCI92xkE0unbSe9eOE8CfzriWqTT5qUh6fS9mN79m86nd6MzaSwZBNLp+xjwN8C/0nsKmjSWvDQknYYkbwGeqqpPJ1kFfCPJ7wF/C/wq8Pz+nSOvqaq9o6xVOhnvPipJjfPSkCQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjfs/Cuq9YzJZ/+oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = tf.constant([[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]], dtype=tf.float32)\n",
    "y_data = tf.constant([[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]], dtype=tf.float32)\n",
    "\n",
    "plt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
    "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "54cebd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal((2, 1)), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal((1,)), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal((2, 1)), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal((1,)), name='bias2')\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal((2, 1)), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal((1,)), name='bias3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "57b9e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hypothesis\n",
    "def neural_net(features):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
    "#     layer3 = tf.concat([layer1, layer2],-1)\n",
    "#     layer3 = tf.reshape(layer3, shape = [-1,2])\n",
    "    hypothesis = tf.sigmoid(tf.matmul(tf.concat([layer1, layer2], 1), W3) + b3)\n",
    "    return hypothesis\n",
    "    \n",
    "# get loss\n",
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -labels*(tf.math.log(hypothesis))-(1-labels)*tf.math.log(1 - hypothesis)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    return cost_mean\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# cut value by decision boundary\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "# get hypothesis\n",
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features),labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1108a58d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6896\n",
      "Iter: 500, Loss: 0.6892\n",
      "Iter: 1000, Loss: 0.6887\n",
      "Iter: 1500, Loss: 0.6882\n",
      "Iter: 2000, Loss: 0.6876\n",
      "Iter: 2500, Loss: 0.6870\n",
      "Iter: 3000, Loss: 0.6862\n",
      "Iter: 3500, Loss: 0.6854\n",
      "Iter: 4000, Loss: 0.6845\n",
      "Iter: 4500, Loss: 0.6834\n",
      "Iter: 5000, Loss: 0.6822\n",
      "Iter: 5500, Loss: 0.6809\n",
      "Iter: 6000, Loss: 0.6794\n",
      "Iter: 6500, Loss: 0.6777\n",
      "Iter: 7000, Loss: 0.6759\n",
      "Iter: 7500, Loss: 0.6738\n",
      "Iter: 8000, Loss: 0.6715\n",
      "Iter: 8500, Loss: 0.6689\n",
      "Iter: 9000, Loss: 0.6661\n",
      "Iter: 9500, Loss: 0.6630\n",
      "Iter: 10000, Loss: 0.6596\n",
      "Iter: 10500, Loss: 0.6559\n",
      "Iter: 11000, Loss: 0.6519\n",
      "Iter: 11500, Loss: 0.6476\n",
      "Iter: 12000, Loss: 0.6430\n",
      "Iter: 12500, Loss: 0.6381\n",
      "Iter: 13000, Loss: 0.6330\n",
      "Iter: 13500, Loss: 0.6276\n",
      "Iter: 14000, Loss: 0.6221\n",
      "Iter: 14500, Loss: 0.6165\n",
      "Iter: 15000, Loss: 0.6108\n",
      "Iter: 15500, Loss: 0.6051\n",
      "Iter: 16000, Loss: 0.5995\n",
      "Iter: 16500, Loss: 0.5939\n",
      "Iter: 17000, Loss: 0.5885\n",
      "Iter: 17500, Loss: 0.5832\n",
      "Iter: 18000, Loss: 0.5781\n",
      "Iter: 18500, Loss: 0.5733\n",
      "Iter: 19000, Loss: 0.5686\n",
      "Iter: 19500, Loss: 0.5642\n",
      "Iter: 20000, Loss: 0.5600\n",
      "Iter: 20500, Loss: 0.5561\n",
      "Iter: 21000, Loss: 0.5523\n",
      "Iter: 21500, Loss: 0.5488\n",
      "Iter: 22000, Loss: 0.5455\n",
      "Iter: 22500, Loss: 0.5423\n",
      "Iter: 23000, Loss: 0.5393\n",
      "Iter: 23500, Loss: 0.5365\n",
      "Iter: 24000, Loss: 0.5338\n",
      "Iter: 24500, Loss: 0.5313\n",
      "Iter: 25000, Loss: 0.5289\n",
      "Iter: 25500, Loss: 0.5266\n",
      "Iter: 26000, Loss: 0.5244\n",
      "Iter: 26500, Loss: 0.5223\n",
      "Iter: 27000, Loss: 0.5202\n",
      "Iter: 27500, Loss: 0.5183\n",
      "Iter: 28000, Loss: 0.5164\n",
      "Iter: 28500, Loss: 0.5145\n",
      "Iter: 29000, Loss: 0.5127\n",
      "Iter: 29500, Loss: 0.5110\n",
      "Iter: 30000, Loss: 0.5092\n",
      "Iter: 30500, Loss: 0.5075\n",
      "Iter: 31000, Loss: 0.5058\n",
      "Iter: 31500, Loss: 0.5041\n",
      "Iter: 32000, Loss: 0.5023\n",
      "Iter: 32500, Loss: 0.5006\n",
      "Iter: 33000, Loss: 0.4988\n",
      "Iter: 33500, Loss: 0.4970\n",
      "Iter: 34000, Loss: 0.4952\n",
      "Iter: 34500, Loss: 0.4933\n",
      "Iter: 35000, Loss: 0.4913\n",
      "Iter: 35500, Loss: 0.4893\n",
      "Iter: 36000, Loss: 0.4871\n",
      "Iter: 36500, Loss: 0.4849\n",
      "Iter: 37000, Loss: 0.4826\n",
      "Iter: 37500, Loss: 0.4801\n",
      "Iter: 38000, Loss: 0.4775\n",
      "Iter: 38500, Loss: 0.4747\n",
      "Iter: 39000, Loss: 0.4718\n",
      "Iter: 39500, Loss: 0.4687\n",
      "Iter: 40000, Loss: 0.4655\n",
      "Iter: 40500, Loss: 0.4621\n",
      "Iter: 41000, Loss: 0.4585\n",
      "Iter: 41500, Loss: 0.4547\n",
      "Iter: 42000, Loss: 0.4506\n",
      "Iter: 42500, Loss: 0.4463\n",
      "Iter: 43000, Loss: 0.4417\n",
      "Iter: 43500, Loss: 0.4368\n",
      "Iter: 44000, Loss: 0.4313\n",
      "Iter: 44500, Loss: 0.4252\n",
      "Iter: 45000, Loss: 0.4182\n",
      "Iter: 45500, Loss: 0.4103\n",
      "Iter: 46000, Loss: 0.4010\n",
      "Iter: 46500, Loss: 0.3902\n",
      "Iter: 47000, Loss: 0.3778\n",
      "Iter: 47500, Loss: 0.3639\n",
      "Iter: 48000, Loss: 0.3487\n",
      "Iter: 48500, Loss: 0.3328\n",
      "Iter: 49000, Loss: 0.3165\n",
      "Iter: 49500, Loss: 0.3002\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50000\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    gradients = grad(x_data, y_data)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(gradients,[W1, W2, W3, b1, b2, b3]))\n",
    "    if i % 500 == 0:\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_fn(neural_net(x_data),y_data)))\n",
    "            \n",
    "test_acc = accuracy_fn(neural_net(x_data),y_data)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_for_everyone",
   "language": "python",
   "name": "ml_for_everyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
